{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Key words using rake and scoring them using word page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "P=0\n",
    "#Question arrays\n",
    "str_arr=[\"After reading the group’s procedure, describe what additional information you would need in order to replicate the experiment. Make sure to include at least three pieces of information.\",\n",
    "        \"Draw a conclusion based on the student’s data. Describe two ways the student could have improved the experimental design and/or validity of the results.\",\n",
    "        \"Explain how pandas in China are similar to koalas in Australia and how they both are different from pythons. Support your response with information from the article.\",\n",
    "        \"Explain the significance of the word “invasive” to the rest of the article. Support your response with information from the article.\",\n",
    "        \"Starting with mRNA leaving the nucleus, list and describe four major steps involved in protein synthesis.\",\n",
    "        \"List and describe three processes used by cells to control the movement of substances across the cell membrane.\",\n",
    "        \"Identify ONE trait that can describe Rose based on her conversations with Anna or Aunt Kolab. Include ONE detail from the story that supports your answer.\",\n",
    "         \"During the story, the reader gets background information about Mr. Leonard. Explain the effect that background information has on Paul. Support your response with details from the story.\",\n",
    "        \"How does the author organize the article? Support your response with details from the article.\",\n",
    "        \"Brandi and Jerry were designing a doghouse. Use the results from the experiment to describe the best paint color for the doghouse.\"]\n",
    "\n",
    "#Calling Rake \n",
    "def extract_rake(str_arr,P):  \n",
    "\n",
    "    \n",
    "    for i in str_arr:\n",
    "        P+=1\n",
    "        from rake_nltk import Rake\n",
    "        r = Rake()\n",
    "        string = i\n",
    "        r.extract_keywords_from_text(string)\n",
    "        terms = []\n",
    "        for x in r.get_ranked_phrases_with_scores():\n",
    "            terms.append(x[1])\n",
    "\n",
    "#using page rank to rank features (paper corrosponding link)\n",
    "        try: \n",
    "            from googlesearch import search \n",
    "        except ImportError: \n",
    "            print(\"No module named 'google' found\") \n",
    "\n",
    "        urls = []\n",
    "        for query in terms:\n",
    "            for j in search(query,lang='en', num=1, stop=5, pause=2):\n",
    "                urls.append(j)\n",
    "   \n",
    "\n",
    "        import nltk, re, pprint\n",
    "        import numpy as np\n",
    "        from nltk import word_tokenize\n",
    "\n",
    "        docs = []\n",
    "\n",
    "        import urllib.parse\n",
    "        import urllib.request\n",
    "        from urllib.error import HTTPError\n",
    "        # import requests \n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "        for url in urls:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "            reg_url = url\n",
    "            req = urllib.request.Request(url=reg_url, headers=headers) \n",
    "        \n",
    "            try:\n",
    "                html = urllib.request.urlopen(req).read()\n",
    "                soup = BeautifulSoup(html)\n",
    "\n",
    "                \n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract()    # rip it out\n",
    "\n",
    "                # get text\n",
    "                text = soup.get_text()\n",
    "\n",
    "                # break into lines and remove leading and trailing space on each\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                # break multi-headlines into a line each\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                # drop blank lines\n",
    "                text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "                text = text.lower()\n",
    "                symbols = \"!\\\"#$%&()*,+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "                for i in symbols:\n",
    "                    text = text.replace(i, ' ')\n",
    "                # print(text)\n",
    "                docs.append(text)\n",
    "            except HTTPError as e:\n",
    "                content = e.read()\n",
    "\n",
    "        print(len(docs))\n",
    "\n",
    "        #import the TfidfVectorizer from Scikit-Learn.  \n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words=None, use_idf=True, norm=None)\n",
    "        transformed_documents = vectorizer.fit_transform(docs)\n",
    "\n",
    "        transformed_documents_as_array = transformed_documents.toarray()\n",
    "        # use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "        print(len(transformed_documents_as_array[1]))\n",
    "\n",
    "        print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "        # print(len(transformed_documents_as_array[0]))\n",
    "\n",
    "        import pandas as pd\n",
    "        for counter, doc in enumerate(transformed_documents_as_array):\n",
    "            # construct a dataframe\n",
    "            tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "            one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "            one_doc_as_df.to_csv('weighted_keywords/set_' + str(P) +'.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rake(str_arr,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Doc2Vec.load('../data/embeddings/enwiki_dbow/doc2vec.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using wikipedia article for the top word to calucatr cosine similarity between response and top word article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "#load pretrained model\n",
    "\n",
    "\n",
    "# Load Data\n",
    "data = pd.read_table('../data/train_rel_2.tsv')\n",
    "print('read_data')\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "score_matrix = np.zeros(shape=(len(data),10))\n",
    "count = -1\n",
    "length = 0\n",
    "sample_res_overlap = []\n",
    "\n",
    "#Load weighted keywords for each prompt (saved by cell above)\n",
    "df1= pd.read_csv('weighted_keywords/set_1.csv')\n",
    "df2 = pd.read_csv('weighted_keywords/set_2.csv')\n",
    "df3= pd.read_csv('weighted_keywords/set_3.csv')\n",
    "df4 = pd.read_csv('weighted_keywords/set_4.csv')\n",
    "df5= pd.read_csv('weighted_keywords/set_5.csv')\n",
    "df6 = pd.read_csv('weighted_keywords/set_6.csv')\n",
    "df7= pd.read_csv('weighted_keywords/set_7.csv')\n",
    "df8 = pd.read_csv('weighted_keywords/set_8.csv')\n",
    "df9= pd.read_csv('weighted_keywords/set_9.csv')\n",
    "df10 = pd.read_csv('weighted_keywords/set_10.csv')\n",
    "\n",
    "for row in data.iterrows():\n",
    "    count = count + 1\n",
    "    sent = row[1][\"EssayText\"]\n",
    "    ans_set = row[1][\"EssaySet\"]\n",
    "    sent = word_tokenize(sent)\n",
    "    sent_vector = model.infer_vector(sent)\n",
    "    flag=False\n",
    "\n",
    "\n",
    "\n",
    "    if ans_set == 1:\n",
    "#choose the top 1 word\n",
    "        top_words = df1['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    " #calling wikipedia for article of the top word\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "#if top word cannot be retrieved using wikipedia pages, use the second top word\n",
    "            article = wikipedia.page(df1['term'].tolist()[1:2]).content\n",
    "\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "#find and append cosine similarity\n",
    "\n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "    if ans_set == 2:\n",
    "        flag=True\n",
    "        if(flag==True)\n",
    "            sample_res_overlap1 = sample_res_overlap.copy()\n",
    "            flag = False\n",
    "\n",
    "        top_words = df2['term'].tolist()[:1]\n",
    "\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df2['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "       \n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "    if ans_set == 3:\n",
    "        flag=True\n",
    "        if(flag==True)\n",
    "            sample_res_overlap2 = sample_res_overlap.copy()\n",
    "            flag = False\n",
    "\n",
    "        top_words = df3['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    "\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except :\n",
    "            article = wikipedia.page(df3['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "  \n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "    if ans_set == 4:\n",
    "        flag=True\n",
    "        if(flag==True)\n",
    "            sample_res_overlap3 = sample_res_overlap.copy()\n",
    "            flag = False\n",
    "\n",
    "        top_words = df4['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    "\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except :\n",
    "            article = wikipedia.page(df4['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "       \n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "    if ans_set == 5:\n",
    "\n",
    "        top_words = df5['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    "\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df5['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "   \n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "    if ans_set == 6:\n",
    "\n",
    "        top_words = df6['term'].tolist()[:1]\n",
    "        \n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df6['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "     \n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "    if ans_set == 7:\n",
    "\n",
    "        top_words = df7['term'].tolist()[:1]\n",
    "\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df7['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "\n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "    if ans_set == 8:\n",
    "\n",
    "        top_words = df8['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    " \n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df8['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "\n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector,article_vec))\n",
    "\n",
    "    if ans_set == 9:\n",
    "        top_words = df9['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    "\n",
    "        try : \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df9['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "    \n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "    if ans_set == 10:\n",
    "        top_words = df10['term'].tolist()[:1]\n",
    "        print(top_words)\n",
    "\n",
    "        try: \n",
    "            article = wikipedia.page(top_words).content\n",
    "        except:\n",
    "            article = wikipedia.page(df10['term'].tolist()[1:2]).content\n",
    "        article = word_tokenize(article.lower())\n",
    "        article_vec = model.infer_vector(article)\n",
    "        sample_res_overlap.append(spatial.distance.cosine(sent_vector, article_vec))\n",
    "\n",
    "        #finallys saving feature\n",
    "np.save(\"features/context_feature\", sample_res_overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
